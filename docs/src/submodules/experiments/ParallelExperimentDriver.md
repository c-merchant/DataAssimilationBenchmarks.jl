# ParallelExperimentDriver 

In order to perform sensitivity testing and estimator tuning, many different parameter
combinations may need to be evaluated for each experiment defined in the submodules 
[GenerateTimeSeries](@ref), [FilterExps](@ref) and [SmootherExps](@ref).  These experiments
are designed so that these hyper-parameter searches can be implemented with naive parallelism,
using [parallel maps](https://en.wikipedia.org/wiki/MapReduce) and Julia's
native [Distributed](https://docs.julialang.org/en/v1/stdlib/Distributed/) computing module.

This module defines argumentless functions to construct an array with each array entry given
by a [NamedTuple](https://docs.julialang.org/en/v1/base/base/#Core.NamedTuple), defining
a particular hyper-parameter configuration.  These functions also define a soft-fail method
for evaluating experiments, with example syntax as
```
args, wrap_exp = method()
```
where the `wrap_exp` follows a convention of

```
function wrap_exp(arguments)
    try
        exp(arguments)
    catch
        print("Error on " * string(arguments) * "\n")
    end
end
```
with `exp` being imported from one of the experiment modules above.

This soft-fail wrapper provides that if a single experiment configuration in the parameter
array fails due to, e.g., numerical overflow, the remaining configurations will continue
their own course unaffected.

## Example usage

An example of how one can use the ParallelExperimentDriver framework to run a sensitivity
test is as follows.  One can define a script as follows:

```
##############################################################################################
module run_sensitivity_test 
##############################################################################################
# imports and exports
using Distributed
@everywhere using DataAssimilationBenchmarks
##############################################################################################

config = ParallelExperimentDriver.my_new_sensitivity_test

print("Generating experiment configurations from " * string(config) * "\n")
print("Generate truth twin\n")

args, wrap_exp = config()
num_exps = length(args)

print("Configuration ready\n")
print("\n")
print("Running " * string(num_exps) * " configurations on " * string(nworkers()) *
      " total workers\n")
print("Begin pmap\n")
pmap(wrap_exp, args)
print("Experiments completed, verify outputs in the appropriate directory under:\n")
print(pkgdir(DataAssimilationBenchmarks) * "/src/data\n")

##############################################################################################
# end module

end
```
where the `my_new_sensitivity_test` is a function that constructs the parameter set and
wrapper as in the other examples in this module.  Running the script using
```
julia -p N run_sensitivity_test.jl
```
will map the evaluation of all parameter configurations to parallel workers where `N`
is the number of workers, to be defined based on the available resources on the user system.
User-defined sensitivity tests can be generated by modifying the above script according
to new constructors defined within the ParallelExperimentDriver module.

## Experiment groups
```@autodocs
Modules = [DataAssimilationBenchmarks.ParallelExperimentDriver]
```
